{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:06:56.420929Z",
     "start_time": "2025-04-29T12:06:55.870211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import torch\n",
    "# from transformers import GemmaTokenizerFast\n",
    "# from transformers import Trainer, TrainingArguments\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# import os"
   ],
   "id": "3172f1549253a104",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:09:09.535522Z",
     "start_time": "2025-04-29T12:06:59.499077Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install --upgrade transformers\n",
   "id": "28bff2db2f7ade16",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\venvs\\thesis\\venv\\lib\\site-packages (4.50.0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in c:\\venvs\\thesis\\venv\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\venvs\\thesis\\venv\\lib\\site-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\venvs\\thesis\\venv\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\venvs\\thesis\\venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\venvs\\thesis\\venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\venvs\\thesis\\venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\venvs\\thesis\\venv\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\venvs\\thesis\\venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\venvs\\thesis\\venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\venvs\\thesis\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\venvs\\thesis\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\venvs\\thesis\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\venvs\\thesis\\venv\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\venvs\\thesis\\venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\venvs\\thesis\\venv\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\venvs\\thesis\\venv\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "   ---------------------------------------- 0.0/10.4 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.6/10.4 MB 8.4 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.9/10.4 MB 10.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.0/10.4 MB 9.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.7/10.4 MB 10.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.4/10.4 MB 10.0 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Installing collected packages: huggingface-hub, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.29.3\n",
      "    Uninstalling huggingface-hub-0.29.3:\n",
      "      Successfully uninstalled huggingface-hub-0.29.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.50.0\n",
      "    Uninstalling transformers-4.50.0:\n",
      "      Successfully uninstalled transformers-4.50.0\n",
      "Successfully installed huggingface-hub-0.30.2 transformers-4.51.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-29T12:09:21.077482Z",
     "start_time": "2025-04-29T12:09:13.120485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Get the absolute path to the project directory\n",
    "project_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# Construct the full path to the dataset\n",
    "data_path = os.path.join(project_dir, \"data\", \"SVAMP.json\")\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_json(data_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "data.head()\n",
    "\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       ID                                               Body  \\\n",
       "0  chal-1  Each pack of dvds costs 76 dollars. If there i...   \n",
       "1  chal-2  Dan had $ 3 left with him after he bought a ca...   \n",
       "2  chal-3  Paco had 26 salty cookies and 17 sweet cookies...   \n",
       "3  chal-4  43 children were riding on the bus. At the bus...   \n",
       "4  chal-5  28 children were riding on the bus. At the bus...   \n",
       "\n",
       "                                            Question         Equation  Answer  \\\n",
       "0      How much do you have to pay to buy each pack?  ( 76.0 - 25.0 )      51   \n",
       "1                   How much did the candy bar cost?    ( 4.0 - 3.0 )       1   \n",
       "2         How many salty cookies did Paco have left?   ( 26.0 - 9.0 )      17   \n",
       "3  How many children got off the bus at the bus s...  ( 43.0 - 21.0 )      22   \n",
       "4  How many more children got on the bus than tho...  ( 30.0 - 28.0 )       2   \n",
       "\n",
       "          Type  \n",
       "0  Subtraction  \n",
       "1  Subtraction  \n",
       "2  Subtraction  \n",
       "3  Subtraction  \n",
       "4  Subtraction  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Body</th>\n",
       "      <th>Question</th>\n",
       "      <th>Equation</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chal-1</td>\n",
       "      <td>Each pack of dvds costs 76 dollars. If there i...</td>\n",
       "      <td>How much do you have to pay to buy each pack?</td>\n",
       "      <td>( 76.0 - 25.0 )</td>\n",
       "      <td>51</td>\n",
       "      <td>Subtraction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chal-2</td>\n",
       "      <td>Dan had $ 3 left with him after he bought a ca...</td>\n",
       "      <td>How much did the candy bar cost?</td>\n",
       "      <td>( 4.0 - 3.0 )</td>\n",
       "      <td>1</td>\n",
       "      <td>Subtraction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chal-3</td>\n",
       "      <td>Paco had 26 salty cookies and 17 sweet cookies...</td>\n",
       "      <td>How many salty cookies did Paco have left?</td>\n",
       "      <td>( 26.0 - 9.0 )</td>\n",
       "      <td>17</td>\n",
       "      <td>Subtraction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chal-4</td>\n",
       "      <td>43 children were riding on the bus. At the bus...</td>\n",
       "      <td>How many children got off the bus at the bus s...</td>\n",
       "      <td>( 43.0 - 21.0 )</td>\n",
       "      <td>22</td>\n",
       "      <td>Subtraction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chal-5</td>\n",
       "      <td>28 children were riding on the bus. At the bus...</td>\n",
       "      <td>How many more children got on the bus than tho...</td>\n",
       "      <td>( 30.0 - 28.0 )</td>\n",
       "      <td>2</td>\n",
       "      <td>Subtraction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T17:41:59.490116Z",
     "start_time": "2025-03-24T17:41:59.429964Z"
    }
   },
   "cell_type": "code",
   "source": "login()",
   "id": "2df3a7200534c117",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15724984696c4d9797a7eadf650c4687"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T17:48:19.791224Z",
     "start_time": "2025-03-24T17:42:17.354716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 3: Load the Gemma 2B Model and Tokenizer\n",
    "model_name = \"google/gemma-2-2b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)"
   ],
   "id": "16aca7bad9302674",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a56494b8e79f4ed8a7c228b321da153f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00001-of-00002.safetensors:  18%|#8        | 923M/4.99G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c3d3e1584bc24eb5bec917b8dbd86452"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\venvs\\thesis\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\DELL\\.cache\\huggingface\\hub\\models--google--gemma-2-2b-it. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "990f9be374e74c22b2fb272f30292414"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cc1a23303ca44228a3de86fb5dbe4398"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b74173bb94984652b72ef96aa50f239f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to offload the whole model to the disk. Please use the `disk_offload` function instead.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 9\u001B[0m\n\u001B[0;32m      7\u001B[0m model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgoogle/gemma-2-2b-it\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      8\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_name)\n\u001B[1;32m----> 9\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mauto\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat16\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\venvs\\thesis\\venv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m    562\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m    563\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n\u001B[1;32m--> 564\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[0;32m    565\u001B[0m         pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39mmodel_args, config\u001B[38;5;241m=\u001B[39mconfig, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    566\u001B[0m     )\n\u001B[0;32m    567\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    568\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    569\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    570\u001B[0m )\n",
      "File \u001B[1;32mC:\\venvs\\thesis\\venv\\lib\\site-packages\\transformers\\modeling_utils.py:4091\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m   4088\u001B[0m         device_map_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moffload_buffers\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   4090\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_fsdp_enabled() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_deepspeed_zero3_enabled():\n\u001B[1;32m-> 4091\u001B[0m         dispatch_model(model, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdevice_map_kwargs)\n\u001B[0;32m   4093\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m hf_quantizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   4094\u001B[0m     hf_quantizer\u001B[38;5;241m.\u001B[39mpostprocess_model(model)\n",
      "File \u001B[1;32mC:\\venvs\\thesis\\venv\\lib\\site-packages\\accelerate\\big_modeling.py:503\u001B[0m, in \u001B[0;36mdispatch_model\u001B[1;34m(model, device_map, main_device, state_dict, offload_dir, offload_index, offload_buffers, skip_keys, preload_module_classes, force_hooks)\u001B[0m\n\u001B[0;32m    501\u001B[0m         model\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m    502\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 503\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    504\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to offload the whole model to the disk. Please use the `disk_offload` function instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    505\u001B[0m         )\n\u001B[0;32m    506\u001B[0m \u001B[38;5;66;03m# Convert OrderedDict back to dict for easier usage\u001B[39;00m\n\u001B[0;32m    507\u001B[0m model\u001B[38;5;241m.\u001B[39mhf_device_map \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(device_map)\n",
      "\u001B[1;31mValueError\u001B[0m: You are trying to offload the whole model to the disk. Please use the `disk_offload` function instead."
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T17:48:19.916354400Z",
     "start_time": "2025-03-23T19:57:19.905548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# split for questions and answers\n",
    "df = pd.DataFrame(data['Body'] + ', ' + data['Question'], columns=['text'])\n",
    "df['label'] = data['Answer']\n",
    "\n",
    "df.head()\n"
   ],
   "id": "663bd7b459ae1e4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                text  label\n",
       "0  Each pack of dvds costs 76 dollars. If there i...     51\n",
       "1  Dan had $ 3 left with him after he bought a ca...      1\n",
       "2  Paco had 26 salty cookies and 17 sweet cookies...     17\n",
       "3  43 children were riding on the bus. At the bus...     22\n",
       "4  28 children were riding on the bus. At the bus...      2"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Each pack of dvds costs 76 dollars. If there i...</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dan had $ 3 left with him after he bought a ca...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Paco had 26 salty cookies and 17 sweet cookies...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43 children were riding on the bus. At the bus...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28 children were riding on the bus. At the bus...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T17:48:19.920367700Z",
     "start_time": "2025-03-24T17:35:20.123588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.model_handlers.perturbator import Perturbator\n",
    "\n",
    "perturbator = Perturbator(model, tokenizer)"
   ],
   "id": "39d2e09e56694fd3",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T17:48:19.978352800Z",
     "start_time": "2025-03-24T17:35:21.793723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = []\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Evaluating\"):\n",
    "    input_text = row[\"text\"]\n",
    "    # Generate answers using the Perturbator\n",
    "    generated_answers = perturbator._generate_samples(\n",
    "        task=\"question-answering\",\n",
    "        input_text=input_text,\n",
    "        num_samples=1,  # Generate one answer per input\n",
    "        temperature=0.7  # Use default temperature\n",
    "    )\n",
    "    # Store results\n",
    "    results.append({\n",
    "        \"input\": input_text,\n",
    "        \"generated_answer\": generated_answers[0],  # Take the first generated answer\n",
    "        \"expected_output\": row[\"output\"]\n",
    "    })"
   ],
   "id": "74151ce054bbaf04",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/1000 [00:00<?, ?it/s]C:\\venvs\\thesis\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\venvs\\thesis\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Evaluating:   0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input length of input_ids is 184, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[26], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m input_text \u001B[38;5;241m=\u001B[39m row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Generate answers using the Perturbator\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m generated_answers \u001B[38;5;241m=\u001B[39m \u001B[43mperturbator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate_samples\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mquestion-answering\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_samples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Generate one answer per input\u001B[39;49;00m\n\u001B[0;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.7\u001B[39;49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Use default temperature\u001B[39;49;00m\n\u001B[0;32m     10\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# Store results\u001B[39;00m\n\u001B[0;32m     12\u001B[0m results\u001B[38;5;241m.\u001B[39mappend({\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput\u001B[39m\u001B[38;5;124m\"\u001B[39m: input_text,\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgenerated_answer\u001B[39m\u001B[38;5;124m\"\u001B[39m: generated_answers[\u001B[38;5;241m0\u001B[39m],  \u001B[38;5;66;03m# Take the first generated answer\u001B[39;00m\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexpected_output\u001B[39m\u001B[38;5;124m\"\u001B[39m: example[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m     16\u001B[0m })\n",
      "File \u001B[1;32m~\\OneDrive\\Dokumenty\\studia\\AI\\Year3\\ThesisAI\\thesis-project\\src\\model_handlers\\perturbator.py:37\u001B[0m, in \u001B[0;36mPerturbator._generate_samples\u001B[1;34m(self, task, input_text, num_samples, temperature, trigger_phrase)\u001B[0m\n\u001B[0;32m     34\u001B[0m     temperature \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefault_temp\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mmodel_type, \u001B[38;5;241m0.7\u001B[39m)\n\u001B[0;32m     36\u001B[0m \u001B[38;5;66;03m# Generate multiple sequences in one call\u001B[39;00m\n\u001B[1;32m---> 37\u001B[0m generated_texts \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     38\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43minput_text\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     39\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     40\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_return_sequences\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_samples\u001B[49m\n\u001B[0;32m     41\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;66;03m# Decode and return the generated texts\u001B[39;00m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39mdecode(text, skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m generated_texts]\n",
      "File \u001B[1;32mC:\\venvs\\thesis\\venv\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\venvs\\thesis\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1905\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[0;32m   1902\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_supports_num_logits_to_keep() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_logits_to_keep\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m model_kwargs:\n\u001B[0;32m   1903\u001B[0m     model_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_logits_to_keep\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m-> 1905\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_generated_length\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_ids_length\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhas_default_max_length\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1907\u001B[0m \u001B[38;5;66;03m# 7. Prepare the cache.\u001B[39;00m\n\u001B[0;32m   1908\u001B[0m \u001B[38;5;66;03m# - `model_kwargs` may be updated in place with a cache as defined by the parameters in `generation_config`.\u001B[39;00m\n\u001B[0;32m   1909\u001B[0m \u001B[38;5;66;03m# - different models have a different cache name expected by the model (default = \"past_key_values\")\u001B[39;00m\n\u001B[0;32m   1910\u001B[0m \u001B[38;5;66;03m# - `max_length`, prepared above, is used to determine the maximum cache length\u001B[39;00m\n\u001B[0;32m   1911\u001B[0m \u001B[38;5;66;03m# TODO (joao): remove `user_defined_cache` after v4.47 (remove default conversion to legacy format)\u001B[39;00m\n\u001B[0;32m   1912\u001B[0m cache_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpast_key_values\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmamba\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcache_params\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32mC:\\venvs\\thesis\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1228\u001B[0m, in \u001B[0;36mGenerationMixin._validate_generated_length\u001B[1;34m(self, generation_config, input_ids_length, has_default_max_length)\u001B[0m\n\u001B[0;32m   1226\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m input_ids_length \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m generation_config\u001B[38;5;241m.\u001B[39mmax_length:\n\u001B[0;32m   1227\u001B[0m     input_ids_string \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdecoder_input_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1228\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1229\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput length of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minput_ids_string\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minput_ids_length\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, but `max_length` is set to\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1230\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgeneration_config\u001B[38;5;241m.\u001B[39mmax_length\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. This can lead to unexpected behavior. You should consider\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1231\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m increasing `max_length` or, better yet, setting `max_new_tokens`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1232\u001B[0m     )\n\u001B[0;32m   1234\u001B[0m \u001B[38;5;66;03m# 2. Min length warnings due to unfeasible parameter combinations\u001B[39;00m\n\u001B[0;32m   1235\u001B[0m min_length_error_suffix \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   1236\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Generation will stop at the defined maximum length. You should decrease the minimum length and/or \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1237\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mincrease the maximum length.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1238\u001B[0m )\n",
      "\u001B[1;31mValueError\u001B[0m: Input length of input_ids is 184, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`."
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate_results(results):\n",
    "    correct = 0\n",
    "    for result in results:\n",
    "        generated_answer = result[\"generated_answer\"].strip().lower()\n",
    "        expected_output = result[\"expected_output\"].strip().lower()\n",
    "        if generated_answer == expected_output:\n",
    "            correct += 1\n",
    "    acc = correct / len(results)\n",
    "    return acc\n",
    "\n",
    "accuracy = evaluate_results(results)\n",
    "print(f\"Model Accuracy on Test Set: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Step 8: Display Sample Results\n",
    "for i, result in enumerate(results[:5]):  # Display first 5 results\n",
    "    print(f\"Example {i + 1}:\")\n",
    "    print(f\"Input: {result['input']}\")\n",
    "    print(f\"Generated Answer: {result['generated_answer']}\")\n",
    "    print(f\"Expected Output: {result['expected_output']}\")\n",
    "    print(\"-\" * 50)"
   ],
   "id": "bd37656d6eaa7c4b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
